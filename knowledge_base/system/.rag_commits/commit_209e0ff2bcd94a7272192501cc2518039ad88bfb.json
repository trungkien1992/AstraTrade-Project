{
  "what_changed": "Refactor  Monolithic Codebase",
  "code_changes": "diff --git a/knowledge_base/backend/config.py b/knowledge_base/backend/config.py\nnew file mode 100644\nindex 0000000..94b5786\n--- /dev/null\n+++ b/knowledge_base/backend/config.py\n@@ -0,0 +1,31 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Configuration module for AstraTrade RAG system\n+\"\"\"\n+\n+# Configuration - Enhanced for AstraTrade with RAGFlow features\n+RAG_CONFIG = {\n+    \"chroma_db_path\": \"../system/chroma_db\",\n+    \"collection_name\": \"astratrade_knowledge_base\",\n+    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n+    \"chunk_size\": 4000,              # Increased from 1000 for better Claude context\n+    \"chunk_overlap\": 800,            # Increased from 200 for better context continuity\n+    \"max_results\": 15,               # Increased from 10 for more comprehensive results\n+    \"similarity_threshold\": 0.7,\n+    \"claude_context_size\": 8000,     # Special large chunks for Claude\n+    \"code_aware_chunking\": True,     # Enable intelligent code chunking\n+    \"template_chunking\": True,       # RAGFlow-inspired template chunking\n+    \"grounded_citations\": True,      # RAGFlow-inspired grounded citations\n+    \"quality_threshold\": 0.7,        # Quality assessment threshold\n+    \"deep_doc_understanding\": True,  # RAGFlow-inspired deep document understanding\n+    \"multi_modal_support\": True,     # Support for heterogeneous data sources\n+    \"platforms\": [                   # AstraTrade supported platforms\n+        \"extended_exchange\",\n+        \"x10_python_sdk\", \n+        \"starknet_dart\",\n+        \"cairo_lang\",\n+        \"avnu_paymaster\",\n+        \"web3auth\",\n+        \"chipi_pay\"\n+    ]\n+}\n\\ No newline at end of file\ndiff --git a/knowledge_base/backend/indexers.py b/knowledge_base/backend/indexers.py\nnew file mode 100644\nindex 0000000..deeba6e\n--- /dev/null\n+++ b/knowledge_base/backend/indexers.py\n@@ -0,0 +1,348 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Platform-specific indexers for AstraTrade RAG system\n+\"\"\"\n+\n+import logging\n+from typing import Dict, Any\n+from models import ProcessedDocument\n+\n+# Logging configuration\n+logger = logging.getLogger(__name__)\n+\n+class PlatformIndexer:\n+    \"\"\"Base class for platform-specific indexers\"\"\"\n+    \n+    def __init__(self, platform_name: str):\n+        self.platform_name = platform_name\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        \"\"\"Index documentation for this platform\"\"\"\n+        return 0\n+\n+class ExtendedExchangeIndexer(PlatformIndexer):\n+    \"\"\"Indexer for Extended Exchange API documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"extended_exchange\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        # Implementation for Extended Exchange API docs\n+        return await self._index_extended_exchange_docs(rag_system)\n+    \n+    async def _index_extended_exchange_docs(self, rag_system) -> int:\n+        \"\"\"Index Extended Exchange API documentation\"\"\"\n+        docs_count = 0\n+        try:\n+            # Process Extended Exchange API documentation\n+            extended_api_content = \"\"\"\n+            Extended Exchange API Documentation\n+            \n+            The Extended Exchange API provides comprehensive trading functionality for professional traders.\n+            \n+            Key Features:\n+            - Real-time market data streaming\n+            - Order management (place, cancel, modify)\n+            - Position tracking and management\n+            - Account balance and margin information\n+            - Historical data access\n+            - WebSocket and REST API endpoints\n+            \n+            Authentication:\n+            - API Key authentication required\n+            - Signature-based request signing\n+            - Rate limiting protection\n+            \n+            Supported Order Types:\n+            - Market orders\n+            - Limit orders\n+            - Stop orders\n+            - Stop-limit orders\n+            \n+            Risk Management:\n+            - Position limits\n+            - Daily loss limits\n+            - Margin requirements\n+            - Liquidation protection\n+            \"\"\"\n+            \n+            doc = ProcessedDocument(\n+                content=extended_api_content,\n+                title=\"Extended Exchange API Documentation\",\n+                category=\"api_documentation\",\n+                subcategory=\"extended_exchange\",\n+                metadata={\n+                    \"platform\": \"extended_exchange\",\n+                    \"doc_type\": \"api_reference\",\n+                    \"importance\": \"critical\",\n+                    \"source\": \"extended_exchange_indexer\"\n+                }\n+            )\n+            \n+            chunks = rag_system._chunk_document(doc)\n+            await rag_system._add_chunks_to_collection(chunks)\n+            docs_count += len(chunks)\n+            \n+        except Exception as e:\n+            logger.error(f\"Failed to index Extended Exchange docs: {str(e)}\")\n+        \n+        return docs_count\n+\n+class X10PythonSDKIndexer(PlatformIndexer):\n+    \"\"\"Indexer for X10 Python SDK documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"x10_python_sdk\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return await self._index_x10_python_docs(rag_system)\n+    \n+    async def _index_x10_python_docs(self, rag_system) -> int:\n+        \"\"\"Index X10 Python SDK documentation\"\"\"\n+        docs_count = 0\n+        try:\n+            x10_sdk_content = \"\"\"\n+            X10 Python SDK Documentation\n+            \n+            The X10 Python SDK provides a comprehensive Python interface for the X10 trading platform.\n+            \n+            Installation:\n+            pip install x10-python-sdk\n+            \n+            Quick Start:\n+            from x10 import Client\n+            \n+            client = Client(api_key=\"your_api_key\", secret=\"your_secret\")\n+            \n+            # Get account balance\n+            balance = client.get_balance()\n+            \n+            # Place an order\n+            order = client.place_order(\n+                symbol=\"BTCUSD\",\n+                side=\"buy\",\n+                quantity=0.1,\n+                price=50000\n+            )\n+            \n+            Key Features:\n+            - Async/await support\n+            - Real-time data streaming\n+            - Order management\n+            - Position tracking\n+            - Risk management tools\n+            - Historical data access\n+            \n+            Error Handling:\n+            - Comprehensive exception handling\n+            - Retry mechanisms\n+            - Rate limit handling\n+            - Connection management\n+            \"\"\"\n+            \n+            doc = ProcessedDocument(\n+                content=x10_sdk_content,\n+                title=\"X10 Python SDK Documentation\",\n+                category=\"sdk_documentation\",\n+                subcategory=\"x10_python\",\n+                metadata={\n+                    \"platform\": \"x10_python_sdk\",\n+                    \"doc_type\": \"sdk_reference\",\n+                    \"importance\": \"high\",\n+                    \"source\": \"x10_python_indexer\"\n+                }\n+            )\n+            \n+            chunks = rag_system._chunk_document(doc)\n+            await rag_system._add_chunks_to_collection(chunks)\n+            docs_count += len(chunks)\n+            \n+        except Exception as e:\n+            logger.error(f\"Failed to index X10 Python SDK docs: {str(e)}\")\n+        \n+        return docs_count\n+\n+class StarknetDartIndexer(PlatformIndexer):\n+    \"\"\"Indexer for Starknet.dart SDK documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"starknet_dart\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return await self._index_starknet_dart_docs(rag_system)\n+    \n+    async def _index_starknet_dart_docs(self, rag_system) -> int:\n+        \"\"\"Index Starknet.dart SDK documentation\"\"\"\n+        docs_count = 0\n+        try:\n+            starknet_dart_content = \"\"\"\n+            Starknet.dart SDK Documentation\n+            \n+            The Starknet.dart SDK enables Flutter/Dart applications to interact with the Starknet blockchain.\n+            \n+            Installation:\n+            dependencies:\n+              starknet: ^latest_version\n+            \n+            Quick Start:\n+            import 'package:starknet/starknet.dart';\n+            \n+            // Initialize provider\n+            final provider = JsonRpcProvider(nodeUrl: 'https://starknet-mainnet.public.blastapi.io');\n+            \n+            // Create account\n+            final account = Account(\n+              provider: provider,\n+              address: 'your_account_address',\n+              keyPair: KeyPair.fromPrivateKey('your_private_key')\n+            );\n+            \n+            Key Features:\n+            - Account management\n+            - Contract interactions\n+            - Transaction signing\n+            - Event filtering\n+            - Type-safe contract calls\n+            - Cairo contract compilation\n+            \n+            Smart Contract Interaction:\n+            - Contract deployment\n+            - Function calls\n+            - Event listening\n+            - State queries\n+            \n+            Security:\n+            - Hardware wallet support\n+            - Secure key management\n+            - Transaction verification\n+            - Network validation\n+            \"\"\"\n+            \n+            doc = ProcessedDocument(\n+                content=starknet_dart_content,\n+                title=\"Starknet.dart SDK Documentation\",\n+                category=\"sdk_documentation\",\n+                subcategory=\"starknet_dart\",\n+                metadata={\n+                    \"platform\": \"starknet_dart\",\n+                    \"doc_type\": \"sdk_reference\",\n+                    \"importance\": \"high\",\n+                    \"source\": \"starknet_dart_indexer\"\n+                }\n+            )\n+            \n+            chunks = rag_system._chunk_document(doc)\n+            await rag_system._add_chunks_to_collection(chunks)\n+            docs_count += len(chunks)\n+            \n+        except Exception as e:\n+            logger.error(f\"Failed to index Starknet.dart SDK docs: {str(e)}\")\n+        \n+        return docs_count\n+\n+class CairoLangIndexer(PlatformIndexer):\n+    \"\"\"Indexer for Cairo language documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"cairo_lang\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return await self._index_cairo_docs(rag_system)\n+    \n+    async def _index_cairo_docs(self, rag_system) -> int:\n+        \"\"\"Index Cairo language documentation\"\"\"\n+        docs_count = 0\n+        try:\n+            cairo_content = \"\"\"\n+            Cairo Language Documentation\n+            \n+            Cairo is a programming language for writing provable programs, where one party can prove to another that \n+            a certain computation was executed correctly.\n+            \n+            Key Concepts:\n+            - Provable computations\n+            - Zero-knowledge proofs\n+            - Starknet smart contracts\n+            - Efficient execution\n+            \n+            Basic Syntax:\n+            #[starknet::contract]\n+            mod HelloStarknet {\n+                #[storage]\n+                struct Storage {\n+                    balance: felt252,\n+                }\n+                \n+                #[external(v0)]\n+                fn increase_balance(ref self: ContractState, amount: felt252) {\n+                    self.balance.write(self.balance.read() + amount);\n+                }\n+                \n+                #[external(v0)]\n+                fn get_balance(self: @ContractState) -> felt252 {\n+                    self.balance.read()\n+                }\n+            }\n+            \n+            Smart Contract Development:\n+            - Contract interfaces\n+            - Storage management\n+            - External functions\n+            - Events and logging\n+            - Access control\n+            \n+            Testing:\n+            - Unit testing framework\n+            - Integration testing\n+            - Deployment testing\n+            - Performance testing\n+            \"\"\"\n+            \n+            doc = ProcessedDocument(\n+                content=cairo_content,\n+                title=\"Cairo Language Documentation\",\n+                category=\"language_documentation\",\n+                subcategory=\"cairo_lang\",\n+                metadata={\n+                    \"platform\": \"cairo_lang\",\n+                    \"doc_type\": \"language_reference\",\n+                    \"importance\": \"high\",\n+                    \"source\": \"cairo_lang_indexer\"\n+                }\n+            )\n+            \n+            chunks = rag_system._chunk_document(doc)\n+            await rag_system._add_chunks_to_collection(chunks)\n+            docs_count += len(chunks)\n+            \n+        except Exception as e:\n+            logger.error(f\"Failed to index Cairo language docs: {str(e)}\")\n+        \n+        return docs_count\n+\n+class AVNUPaymasterIndexer(PlatformIndexer):\n+    \"\"\"Indexer for AVNU Paymaster documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"avnu_paymaster\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return 0  # Placeholder\n+\n+class Web3AuthIndexer(PlatformIndexer):\n+    \"\"\"Indexer for Web3Auth documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"web3auth\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return 0  # Placeholder\n+\n+class ChipiPayIndexer(PlatformIndexer):\n+    \"\"\"Indexer for ChipiPay SDK documentation\"\"\"\n+    \n+    def __init__(self):\n+        super().__init__(\"chipi_pay\")\n+    \n+    async def index_platform_docs(self, rag_system) -> int:\n+        return 0  # Placeholder\n\\ No newline at end of file\ndiff --git a/knowledge_base/backend/main.py b/knowledge_base/backend/main.py\nindex 2245435..6452eaf 100644\n--- a/knowledge_base/backend/main.py\n+++ b/knowledge_base/backend/main.py\n@@ -15,2 +14,0 @@ RAGFlow-inspired features:\n-import os\n-import asyncio\n@@ -18 +15,0 @@ import time\n-import hashlib\n@@ -20,4 +17,3 @@ import logging\n-from typing import List, Optional, Dict, Any, Union\n-from dataclasses import dataclass\n-from datetime import datetime\n-from pathlib import Path\n+from typing import Dict, Any\n+from chromadb.utils import embedding_functions\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\n@@ -27 +22,0 @@ from fastapi.middleware.cors import CORSMiddleware\n-from pydantic import BaseModel, Field\n@@ -30,9 +25,4 @@ import uvicorn\n-# Vector database and embeddings\n-import chromadb\n-from sentence_transformers import SentenceTransformer\n-from chromadb.utils import embedding_functions\n-\n-# Document processing\n-import requests\n-from langchain.text_splitter import RecursiveCharacterTextSplitter\n-from langchain.schema import Document\n+# Local imports\n+from config import RAG_CONFIG\n+from models import QueryRequest, QueryResponse, IndexRequest, IndexResponse, StatsResponse\n+from rag_system import AstraTradeRAG\n@@ -59,723 +48,0 @@ logger = logging.getLogger(__name__)\n-# Configuration - Enhanced for AstraTrade with RAGFlow features\n-RAG_CONFIG = {\n-    \"chroma_db_path\": \"../system/chroma_db\",\n-    \"collection_name\": \"astratrade_knowledge_base\",\n-    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n-    \"chunk_size\": 4000,              # Increased from 1000 for better Claude context\n-    \"chunk_overlap\": 800,            # Increased from 200 for better context continuity\n-    \"max_results\": 15,               # Increased from 10 for more comprehensive results\n-    \"similarity_threshold\": 0.7,\n-    \"claude_context_size\": 8000,     # Special large chunks for Claude\n-    \"code_aware_chunking\": True,     # Enable intelligent code chunking\n-    \"template_chunking\": True,       # RAGFlow-inspired template chunking\n-    \"grounded_citations\": True,      # RAGFlow-inspired grounded citations\n-    \"quality_threshold\": 0.7,        # Quality assessment threshold\n-    \"deep_doc_understanding\": True,  # RAGFlow-inspired deep document understanding\n-    \"multi_modal_support\": True,     # Support for heterogeneous data sources\n-    \"platforms\": [                   # AstraTrade supported platforms\n-        \"extended_exchange\",\n-        \"x10_python_sdk\", \n-        \"starknet_dart\",\n-        \"cairo_lang\",\n-        \"avnu_paymaster\",\n-        \"web3auth\",\n-        \"chipi_pay\"\n-    ]\n-}\n-\n-# Pydantic models\n-class QueryRequest(BaseModel):\n-    query: str\n-    max_results: Optional[int] = 5\n-    category: Optional[str] = None\n-    min_similarity: Optional[float] = 0.6\n-\n-class QueryResponse(BaseModel):\n-    results: List[Dict[str, Any]]\n-    query_time: float\n-    total_results: int\n-\n-class IndexRequest(BaseModel):\n-    force_reindex: bool = False\n-\n-class IndexResponse(BaseModel):\n-    status: str\n-    documents_indexed: int\n-    time_taken: float\n-\n-class StatsResponse(BaseModel):\n-    total_documents: int\n-    categories: Dict[str, int]\n-    last_updated: str\n-    embedding_model: str\n-\n-@dataclass\n-class ProcessedDocument:\n-    content: str\n-    title: str\n-    category: str\n-    subcategory: Optional[str]\n-    metadata: Dict[str, Any]\n-    source_url: Optional[str] = None\n-\n-class AstraTradeRAG:\n-    \"\"\"High-performance RAG system for AstraTrade trading platform with RAGFlow features\"\"\"\n-    \n-    def __init__(self):\n-        self.chroma_client = None\n-        self.collection = None\n-        self.embedding_model = None\n-        self.text_splitter = None\n-        self.documents_indexed = 0\n-        self.last_updated = None\n-        self.document_cache = {}          # Cache for processed documents\n-        self.quality_assessor = None      # RAGFlow-inspired quality assessment\n-        self.citation_tracker = {}        # Track citations for grounded answers\n-        self.platform_indexers = {}       # Platform-specific indexers\n-        \n-    async def initialize(self):\n-        \"\"\"Initialize the RAG system with RAGFlow-inspired features\"\"\"\n-        logger.info(\"ðŸš€ Initializing AstraTrade RAG system with advanced features...\")\n-        \n-        # Initialize ChromaDB\n-        self.chroma_client = chromadb.PersistentClient(path=RAG_CONFIG[\"chroma_db_path\"])\n-        \n-        # Initialize embedding model\n-        self.embedding_model = SentenceTransformer(RAG_CONFIG[\"embedding_model\"])\n-        \n-        # Create embedding function for ChromaDB\n-        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n-            model_name=RAG_CONFIG[\"embedding_model\"]\n-        )\n-        \n-        # Get or create collection\n-        try:\n-            self.collection = self.chroma_client.get_collection(\n-                name=RAG_CONFIG[\"collection_name\"],\n-                embedding_function=embedding_function\n-            )\n-            logger.info(f\"âœ… Loaded existing collection: {RAG_CONFIG['collection_name']}\")\n-        except Exception:\n-            self.collection = self.chroma_client.create_collection(\n-                name=RAG_CONFIG[\"collection_name\"],\n-                embedding_function=embedding_function,\n-                metadata={\n-                    \"description\": \"AstraTrade multi-platform trading knowledge base\",\n-                    \"platforms\": RAG_CONFIG[\"platforms\"],\n-                    \"ragflow_features\": {\n-                        \"template_chunking\": RAG_CONFIG[\"template_chunking\"],\n-                        \"grounded_citations\": RAG_CONFIG[\"grounded_citations\"],\n-                        \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n-                    }\n-                }\n-            )\n-            logger.info(f\"âœ… Created new collection: {RAG_CONFIG['collection_name']}\")\n-        \n-        # Initialize text splitter with RAGFlow-inspired improvements\n-        self.text_splitter = RecursiveCharacterTextSplitter(\n-            chunk_size=RAG_CONFIG[\"chunk_size\"],\n-            chunk_overlap=RAG_CONFIG[\"chunk_overlap\"],\n-            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n-            length_function=len,\n-            is_separator_regex=False\n-        )\n-        \n-        # Initialize platform-specific indexers\n-        self._initialize_platform_indexers()\n-        \n-        # Initialize quality assessor\n-        self._initialize_quality_assessor()\n-        \n-        # Get current stats\n-        count = self.collection.count()\n-        self.documents_indexed = count\n-        logger.info(f\"ðŸ“Š Current documents in collection: {count}\")\n-        logger.info(f\"ðŸ”§ RAGFlow features enabled: {list(RAG_CONFIG.keys())}\")\n-        \n-    def _initialize_platform_indexers(self):\n-        \"\"\"Initialize platform-specific indexers for AstraTrade\"\"\"\n-        self.platform_indexers = {\n-            \"extended_exchange\": ExtendedExchangeIndexer(),\n-            \"x10_python_sdk\": X10PythonSDKIndexer(),\n-            \"starknet_dart\": StarknetDartIndexer(),\n-            \"cairo_lang\": CairoLangIndexer(),\n-            \"avnu_paymaster\": AVNUPaymasterIndexer(),\n-            \"web3auth\": Web3AuthIndexer(),\n-            \"chipi_pay\": ChipiPayIndexer()\n-        }\n-        \n-    def _initialize_quality_assessor(self):\n-        \"\"\"Initialize RAGFlow-inspired quality assessment\"\"\"\n-        self.quality_assessor = DocumentQualityAssessor(\n-            threshold=RAG_CONFIG[\"quality_threshold\"],\n-            platforms=RAG_CONFIG[\"platforms\"]\n-        )\n-        \n-    async def index_astratrade_documentation(self, force_reindex: bool = False) -> Dict[str, Any]:\n-        \"\"\"Index all AstraTrade documentation with RAGFlow-inspired features\"\"\"\n-        start_time = datetime.now()\n-        \n-        if force_reindex:\n-            logger.info(\"ðŸ”„ Force reindexing - clearing existing collection...\")\n-            self.collection.delete()\n-            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n-                model_name=RAG_CONFIG[\"embedding_model\"]\n-            )\n-            self.collection = self.chroma_client.create_collection(\n-                name=RAG_CONFIG[\"collection_name\"],\n-                embedding_function=embedding_function,\n-                metadata={\n-                    \"description\": \"AstraTrade multi-platform trading knowledge base\",\n-                    \"platforms\": RAG_CONFIG[\"platforms\"],\n-                    \"ragflow_features\": {\n-                        \"template_chunking\": RAG_CONFIG[\"template_chunking\"],\n-                        \"grounded_citations\": RAG_CONFIG[\"grounded_citations\"],\n-                        \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n-                    }\n-                }\n-            )\n-        \n-        # Collect documents from all platforms\n-        platform_results = await self._index_all_platforms()\n-        \n-        # Apply RAGFlow-inspired quality assessment\n-        quality_report = await self._assess_collection_quality()\n-        \n-        # Update stats\n-        total_docs = sum(platform_results.values())\n-        self.documents_indexed = total_docs\n-        self.last_updated = datetime.now().isoformat()\n-        \n-        time_taken = (datetime.now() - start_time).total_seconds()\n-        \n-        return {\n-            \"status\": \"completed\",\n-            \"documents_indexed\": total_docs,\n-            \"platform_breakdown\": platform_results,\n-            \"quality_report\": quality_report,\n-            \"ragflow_features\": {\n-                \"template_chunking_applied\": RAG_CONFIG[\"template_chunking\"],\n-                \"grounded_citations_enabled\": RAG_CONFIG[\"grounded_citations\"],\n-                \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n-            },\n-            \"time_taken\": time_taken\n-        }\n-    \n-    async def _index_all_platforms(self) -> Dict[str, int]:\n-        \"\"\"Index documentation from all AstraTrade platforms\"\"\"\n-        platform_results = {}\n-        \n-        for platform_name, indexer in self.platform_indexers.items():\n-            try:\n-                logger.info(f\"ðŸ” Indexing {platform_name} documentation...\")\n-                docs_count = await indexer.index_platform_docs(self)\n-                platform_results[platform_name] = docs_count\n-                logger.info(f\"âœ… Indexed {docs_count} documents from {platform_name}\")\n-            except Exception as e:\n-                logger.error(f\"âŒ Failed to index {platform_name}: {str(e)}\")\n-                platform_results[platform_name] = 0\n-        \n-        return platform_results\n-    \n-    async def _assess_collection_quality(self) -> Dict[str, Any]:\n-        \"\"\"Assess the quality of the indexed collection - RAGFlow inspired\"\"\"\n-        if not self.quality_assessor:\n-            return {\"status\": \"quality_assessor_not_initialized\"}\n-        \n-        try:\n-            # Get sample of documents for quality assessment\n-            sample_results = self.collection.get(limit=100, include=[\"documents\", \"metadatas\"])\n-            \n-            quality_metrics = {\n-                \"total_documents\": len(sample_results[\"documents\"]),\n-                \"platform_coverage\": {},\n-                \"chunk_quality_distribution\": {},\n-                \"citation_completeness\": 0.0,\n-                \"template_coverage\": 0.0\n-            }\n-            \n-            # Analyze platform coverage\n-            for metadata in sample_results[\"metadatas\"]:\n-                platform = metadata.get(\"platform\", \"unknown\")\n-                quality_metrics[\"platform_coverage\"][platform] = quality_metrics[\"platform_coverage\"].get(platform, 0) + 1\n-            \n-            # Analyze chunk quality\n-            quality_scores = []\n-            for doc, metadata in zip(sample_results[\"documents\"], sample_results[\"metadatas\"]):\n-                quality_score = self.quality_assessor.assess_document_quality(doc, metadata)\n-                quality_scores.append(quality_score)\n-            \n-            if quality_scores:\n-                quality_metrics[\"average_quality_score\"] = sum(quality_scores) / len(quality_scores)\n-                quality_metrics[\"high_quality_percentage\"] = len([s for s in quality_scores if s > 0.8]) / len(quality_scores) * 100\n-            \n-            return quality_metrics\n-            \n-        except Exception as e:\n-            logger.error(f\"Quality assessment failed: {str(e)}\")\n-            return {\"status\": \"assessment_failed\", \"error\": str(e)}\n-    \n-    async def _collect_astratrade_documentation(self) -> List[ProcessedDocument]:\n-        \"\"\"Collect all AstraTrade platform documentation with RAGFlow-inspired deep understanding\"\"\"\n-        documents = []\n-        \n-        # Manual documentation files\n-        documents.extend(await self._fetch_manual_docs())\n-        \n-        # Extended Exchange API documentation\n-        documents.extend(await self._fetch_extended_exchange_docs())\n-        \n-        # X10 Python SDK documentation\n-        documents.extend(await self._fetch_x10_python_docs())\n-        \n-        # Starknet.dart SDK documentation\n-        documents.extend(await self._fetch_starknet_dart_docs())\n-        \n-        # Cairo language documentation\n-        documents.extend(await self._fetch_cairo_docs())\n-        \n-        # AVNU Paymaster documentation\n-        documents.extend(await self._fetch_avnu_paymaster_docs())\n-        \n-        # Web3Auth documentation\n-        documents.extend(await self._fetch_web3auth_docs())\n-        \n-        # ChipiPay SDK documentation\n-        documents.extend(await self._fetch_chipi_pay_docs())\n-        \n-        return documents\n-    \n-    async def _fetch_manual_docs(self) -> List[ProcessedDocument]:\n-        \"\"\"Fetch manual documentation files from the docs folder\"\"\"\n-        documents = []\n-        docs_path = Path(\"../docs/manual_docs\")\n-        \n-        if docs_path.exists():\n-            for doc_file in docs_path.glob(\"*.md\"):\n-                try:\n-                    content = doc_file.read_text(encoding='utf-8')\n-                    \n-                    # Apply deep document understanding\n-                    doc_type = self._detect_document_type(content, doc_file.name)\n-                    importance = self._assess_document_importance(content, doc_file.name)\n-                    \n-                    documents.append(ProcessedDocument(\n-                        content=content,\n-                        title=doc_file.stem.replace('_', ' ').title(),\n-                        category=doc_type[\"category\"],\n-                        subcategory=doc_type[\"subcategory\"],\n-                        metadata={\n-                            \"source\": \"manual_docs\",\n-                            \"file_path\": str(doc_file),\n-                            \"importance\": importance,\n-                            \"doc_type\": doc_type[\"type\"],\n-                            \"platform\": doc_type[\"platform\"],\n-                            \"last_modified\": doc_file.stat().st_mtime\n-                        }\n-                    ))\n-                    \n-                except Exception as e:\n-                    logger.error(f\"Failed to process {doc_file}: {str(e)}\")\n-        \n-        return documents\n-    \n-    def _detect_document_type(self, content: str, filename: str) -> Dict[str, str]:\n-        \"\"\"Detect document type using RAGFlow-inspired analysis\"\"\"\n-        content_lower = content.lower()\n-        filename_lower = filename.lower()\n-        \n-        # Platform detection\n-        if \"extended\" in filename_lower and \"api\" in filename_lower:\n-            return {\n-                \"category\": \"api_documentation\",\n-                \"subcategory\": \"extended_exchange\",\n-                \"type\": \"api_reference\",\n-                \"platform\": \"extended_exchange\"\n-            }\n-        elif \"x10\" in filename_lower or \"python\" in filename_lower:\n-            return {\n-                \"category\": \"sdk_documentation\",\n-                \"subcategory\": \"x10_python\",\n-                \"type\": \"sdk_reference\",\n-                \"platform\": \"x10_python_sdk\"\n-            }\n-        elif \"starknet\" in filename_lower or \"dart\" in filename_lower:\n-            return {\n-                \"category\": \"sdk_documentation\",\n-                \"subcategory\": \"starknet_dart\",\n-                \"type\": \"sdk_reference\",\n-                \"platform\": \"starknet_dart\"\n-            }\n-        elif \"cairo\" in filename_lower:\n-            return {\n-                \"category\": \"language_documentation\",\n-                \"subcategory\": \"cairo_lang\",\n-                \"type\": \"language_reference\",\n-                \"platform\": \"cairo_lang\"\n-            }\n-        elif \"avnu\" in filename_lower or \"paymaster\" in filename_lower:\n-            return {\n-                \"category\": \"integration_documentation\",\n-                \"subcategory\": \"avnu_paymaster\",\n-                \"type\": \"integration_guide\",\n-                \"platform\": \"avnu_paymaster\"\n-            }\n-        elif \"web3auth\" in filename_lower:\n-            return {\n-                \"category\": \"authentication_documentation\",\n-                \"subcategory\": \"web3auth\",\n-                \"type\": \"auth_reference\",\n-                \"platform\": \"web3auth\"\n-            }\n-        elif \"chipi\" in filename_lower:\n-            return {\n-                \"category\": \"payment_documentation\",\n-                \"subcategory\": \"chipi_pay\",\n-                \"type\": \"payment_reference\",\n-                \"platform\": \"chipi_pay\"\n-            }\n-        else:\n-            return {\n-                \"category\": \"general_documentation\",\n-                \"subcategory\": \"general\",\n-                \"type\": \"general_reference\",\n-                \"platform\": \"general\"\n-            }\n-    \n-    def _assess_document_importance(self, content: str, filename: str) -> str:\n-        \"\"\"Assess document importance using RAGFlow-inspired analysis\"\"\"\n-        content_lower = content.lower()\n-        \n-        # Critical indicators\n-        critical_indicators = [\"api reference\", \"getting started\", \"authentication\", \"security\", \"deployment\"]\n-        if any(indicator in content_lower for indicator in critical_indicators):\n-            return \"critical\"\n-        \n-        # High importance indicators\n-        high_indicators = [\"integration\", \"configuration\", \"examples\", \"tutorial\", \"guide\"]\n-        if any(indicator in content_lower for indicator in high_indicators):\n-            return \"high\"\n-        \n-        # Medium importance indicators\n-        medium_indicators = [\"reference\", \"documentation\", \"sdk\", \"api\"]\n-        if any(indicator in content_lower for indicator in medium_indicators):\n-            return \"medium\"\n-        \n-        return \"low\"\n-    \n-    async def _fetch_github_docs(self) -> List[ProcessedDocument]:\n-        \"\"\"Fetch documentation from GitHub repository\"\"\"\n-        documents = []\n-        \n-        # GitHub API endpoints for starknet.dart\n-        base_url = \"https://api.github.com/repos/focustree/starknet.dart\"\n-        \n-        # README\n-        documents.append(ProcessedDocument(\n-            content=await self._fetch_github_file(f\"{base_url}/contents/README.md\"),\n-            title=\"Starknet.dart SDK README\",\n-            category=\"overview\",\n-            subcategory=\"readme\",\n-            metadata={\"source\": \"github\", \"importance\": \"critical\"},\n-            source_url=\"https://github.com/focustree/starknet.dart/blob/main/README.md\"\n-        ))\n-        \n-        # Documentation files\n-        doc_files = [\n-            \"CONTRIBUTING.md\",\n-            \"CHANGELOG.md\",\n-            \"docs/getting-started.md\",\n-            \"docs/api-reference.md\",\n-            \"docs/examples.md\",\n-        ]\n-        \n-        for file_path in doc_files:\n-            try:\n-                content = await self._fetch_github_file(f\"{base_url}/contents/{file_path}\")\n-                if content:\n-                    documents.append(ProcessedDocument(\n-                        content=content,\n-                        title=f\"Starknet.dart {file_path}\",\n-                        category=\"documentation\",\n-                        subcategory=file_path.split(\"/\")[-1].replace(\".md\", \"\"),\n-                        metadata={\"source\": \"github\", \"file_path\": file_path},\n-                        source_url=f\"https://github.com/focustree/starknet.dart/blob/main/{file_path}\"\n-                    ))\n-            except Exception as e:\n-                print(f\"âš ï¸  Could not fetch {file_path}: {e}\")\n-        \n-        return documents\n-    \n-    async def _fetch_github_file(self, url: str) -> Optional[str]:\n-        \"\"\"Fetch a file from GitHub API\"\"\"\n-        try:\n-            response = requests.get(url)\n-            if response.status_code == 200:\n-                data = response.json()\n-                if data.get(\"content\"):\n-                    import base64\n-                    content = base64.b64decode(data[\"content\"]).decode(\"utf-8\")\n-                    return content\n-        except Exception as e:\n-            print(f\"Error fetching GitHub file: {e}\")\n-        return None\n-    \n-    async def _fetch_pubdev_docs(self) -> List[ProcessedDocument]:\n-        \"\"\"Fetch API documentation from pub.dev\"\"\"\n-        documents = []\n-        \n-        # API documentation from pub.dev\n-        packages = [\n-            \"starknet\",\n-            \"starknet_provider\",\n-            \"wallet_kit\",\n-            \"secure_store\",\n-            \"avnu_paymaster_provider\",\n-            \"starknet_builder\"\n-        ]\n-        \n-        for package in packages:\n-            try:\n-                # Fetch package info\n-                url = f\"https://pub.dev/api/packages/{package}\"\n-                response = requests.get(url)\n-                if response.status_code == 200:\n-                    data = response.json()\n-                    description = data.get(\"latest\", {}).get(\"pubspec\", {}).get(\"description\", \"\")\n-                    \n-                    documents.append(ProcessedDocument(\n-                        content=f\"Package: {package}\\nDescription: {description}\\n\"\n-                                f\"Latest Version: {data.get('latest', {}).get('version', '')}\\n\"\n-                                f\"Published: {data.get('latest', {}).get('published', '')}\\n\"\n-                                f\"Dependencies: {data.get('latest', {}).get('pubspec', {}).get('dependencies', {})}\",\n-                        title=f\"{package} Package Info\",\n-                        category=\"packages\",\n-                        subcategory=package,\n-                        metadata={\"source\": \"pub.dev\", \"package\": package},\n-                        source_url=f\"https://pub.dev/packages/{package}\"\n-                    ))\n-            except Exception as e:\n-                print(f\"âš ï¸  Could not fetch package {package}: {e}\")\n-        \n-        return documents\n-    \n-    async def _fetch_official_docs(self) -> List[ProcessedDocument]:\n-        \"\"\"Fetch documentation from official website\"\"\"\n-        documents = []\n-        \n-        # Official documentation site content\n-        official_content = \"\"\"\n-        Starknet.dart SDK Official Documentation\n-        \n-        The goal of this SDK is to be able to interact with StarkNet smart contracts in a type-safe way.\n-        You can also call the JSON-RPC endpoint exposed by the Starknet full nodes.\n-        \n-        The priority is to build the best possible Starknet SDK for dart applications,\n-        thus unlocking the era of Flutter mobile apps on Starknet.\n-        \n-        Supported Features:\n-        - Invoke transactions (versions 0, 1, 3)\n-        - Declare transactions (versions 1, 2, 3)\n-        - Deploy Account transactions (versions 1, 3)\n-        - JSON RPC version 0.7.1 support\n-        - Type-safe contract interactions\n-        - Mobile-first development approach\n-        \n-        Key Packages:\n-        - Starknet: Core SDK functionality\n-        - Starknet Provider: Network provider implementations\n-        - Wallet Kit: Wallet integration utilities\n-        - Secure Store: Secure storage for keys and credentials\n-        - Avnu Paymaster Provider: Paymaster integration\n-        - Starknet Builder: Development tools\n-        \"\"\"\n-        \n-        documents.append(ProcessedDocument(\n-            content=official_content,\n-            title=\"Official Starknet.dart Documentation\",\n-            category=\"documentation\",\n-            subcategory=\"official\",\n-            metadata={\"source\": \"official_site\", \"importance\": \"critical\"},\n-            source_url=\"https://starknetdart.dev\"\n-        ))\n-        \n-        return documents\n-    \n-    async def _fetch_example_projects(self) -> List[ProcessedDocument]:\n-        \"\"\"Fetch example project documentation\"\"\"\n-        documents = []\n-        \n-        examples = [\n-            {\n-                \"title\": \"NFT Marketplace Example\",\n-                \"content\": \"\"\"\n-                Complete NFT marketplace implementation using Starknet.dart SDK.\n-                \n-                Features:\n-                - ERC-721 token standard\n-                - Minting and trading\n-                - Flutter mobile interface\n-                - Wallet integration\n-                - Smart contract deployment\n-                \n-                Architecture:\n-                - Cairo smart contracts\n-                - Flutter frontend with Riverpod\n-                - Starknet.dart SDK integration\n-                - Secure key management\n-                \n-                Usage:\n-                - Connect to Starknet testnet\n-                - Deploy NFT contracts\n-                - Mint and trade NFTs\n-                - Manage wallet accounts\n-                \"\"\",\n-                \"category\": \"examples\",\n-                \"subcategory\": \"nft_marketplace\"\n-            },\n-            {\n-                \"title\": \"Mobile Wallet Example\",\n-                \"content\": \"\"\"\n-                Full-featured mobile wallet for Starknet.\n-                \n-                Features:\n-                - Account management\n-                - Token transfers\n-                - Transaction history\n-                - Secure storage\n-                - Biometric authentication\n-                \n-                Implementation:\n-                - Flutter cross-platform\n-                - Hardware-backed security\n-                - Multi-account support\n-                - Real-time balance updates\n-                - Push notifications\n-                \n-                Security:\n-                - Hardware security module\n-                - Biometric locks\n-                - Encrypted storage\n-                - Secure key derivation\n-                \"\"\",\n-                \"category\": \"examples\",\n-                \"subcategory\": \"wallet\"\n-            }\n-        ]\n-        \n-        for example in examples:\n-            documents.append(ProcessedDocument(\n-                content=example[\"content\"],\n-                title=example[\"title\"],\n-                category=example[\"category\"],\n-                subcategory=example[\"subcategory\"],\n-                metadata={\"source\": \"examples\", \"complexity\": \"advanced\"}\n-            ))\n-        \n-        return documents\n-    \n-    def _chunk_document(self, doc: ProcessedDocument) -> List[Dict[str, Any]]:\n-        \"\"\"Chunk a document into smaller pieces\"\"\"\n-        chunks = self.text_splitter.split_text(doc.content)\n-        \n-        chunked_docs = []\n-        for i, chunk in enumerate(chunks):\n-            chunk_id = f\"{doc.category}_{doc.subcategory}_{i}\" if doc.subcategory else f\"{doc.category}_{i}\"\n-            \n-            chunked_docs.append({\n-                \"id\": chunk_id,\n-                \"content\": chunk,\n-                \"metadata\": {\n-                    \"title\": doc.title,\n-                    \"category\": doc.category,\n-                    \"subcategory\": doc.subcategory,\n-                    \"chunk_index\": i,\n-                    \"total_chunks\": len(chunks),\n-                    \"source_url\": doc.source_url,\n-                    **doc.metadata\n-                }\n-            })\n-        \n-        return chunked_docs\n-    \n-    async def _add_chunks_to_collection(self, chunks: List[Dict[str, Any]]):\n-        \"\"\"Add document chunks to ChromaDB collection\"\"\"\n-        batch_size = 100\n-        \n-        for i in range(0, len(chunks), batch_size):\n-            batch = chunks[i:i + batch_size]\n-            \n-            ids = [chunk[\"id\"] for chunk in batch]\n-            documents = [chunk[\"content\"] for chunk in batch]\n-            metadatas = [chunk[\"metadata\"] for chunk in batch]\n-            \n-            self.collection.add(\n-                ids=ids,\n-                documents=documents,\n-                metadatas=metadatas\n-            )\n-            \n-            print(f\"âœ… Added batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}\")\n-    \n-    async def search(self, query: str, max_results: int = 5, category: str = None, \n-                    min_similarity: float = 0.6) -> Dict[str, Any]:\n-        \"\"\"Search the knowledge base\"\"\"\n-        start_time = datetime.now()\n-        \n-        # Prepare query filters\n-        where_filter = {}\n-        if category:\n-            where_filter[\"category\"] = category\n-        \n-        # Perform vector search\n-        results = self.collection.query(\n-            query_texts=[query],\n-            n_results=max_results,\n-            where=where_filter if where_filter else None\n-        )\n-        \n-        # Process results\n-        processed_results = []\n-        if results[\"documents\"] and results[\"documents\"][0]:\n-            for i, doc in enumerate(results[\"documents\"][0]):\n-                metadata = results[\"metadatas\"][0][i] if results[\"metadatas\"] else {}\n-                distance = results[\"distances\"][0][i] if results[\"distances\"] else 0\n-                similarity = 1 - distance  # Convert distance to similarity\n-                \n-                if similarity >= min_similarity:\n-                    processed_results.append({\n-                        \"content\": doc,\n-                        \"title\": metadata.get(\"title\", \"Unknown\"),\n-                        \"category\": metadata.get(\"category\", \"Unknown\"),\n-                        \"subcategory\": metadata.get(\"subcategory\"),\n-                        \"similarity\": similarity,\n-                        \"source_url\": metadata.get(\"source_url\"),\n-                        \"metadata\": metadata\n-                    })\n-        \n-        query_time = (datetime.now() - start_time).total_seconds()\n-        \n-        return {\n-            \"results\": processed_results,\n-            \"query_time\": query_time,\n-            \"total_results\": len(processed_results)\n-        }\n-    \n-    def get_stats(self) -> Dict[str, Any]:\n-        \"\"\"Get knowledge base statistics\"\"\"\n-        total_docs = self.collection.count()\n-        \n-        # Get categories\n-        all_metadata = self.collection.get()[\"metadatas\"]\n-        categories = {}\n-        for metadata in all_metadata:\n-            category = metadata.get(\"category\", \"unknown\")\n-            categories[category] = categories.get(category, 0) + 1\n-        \n-        return {\n-            \"total_documents\": total_docs,\n-            \"categories\": categories,\n-            \"last_updated\": self.last_updated or \"Never\",\n-            \"embedding_model\": RAG_CONFIG[\"embedding_model\"]\n-        }\n-\n@@ -798,377 +64,0 @@ app.add_middleware(\n-# Platform-specific indexers for AstraTrade\n-class PlatformIndexer:\n-    \"\"\"Base class for platform-specific indexers\"\"\"\n-    \n-    def __init__(self, platform_name: str):\n-        self.platform_name = platform_name\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        \"\"\"Index documentation for this platform\"\"\"\n-        return 0\n-\n-class ExtendedExchangeIndexer(PlatformIndexer):\n-    \"\"\"Indexer for Extended Exchange API documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"extended_exchange\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        # Implementation for Extended Exchange API docs\n-        return await self._index_extended_exchange_docs(rag_system)\n-    \n-    async def _index_extended_exchange_docs(self, rag_system) -> int:\n-        \"\"\"Index Extended Exchange API documentation\"\"\"\n-        docs_count = 0\n-        try:\n-            # Process Extended Exchange API documentation\n-            extended_api_content = \"\"\"\n-            Extended Exchange API Documentation\n-            \n-            The Extended Exchange API provides comprehensive trading functionality for professional traders.\n-            \n-            Key Features:\n-            - Real-time market data streaming\n-            - Order management (place, cancel, modify)\n-            - Position tracking and management\n-            - Account balance and margin information\n-            - Historical data access\n-            - WebSocket and REST API endpoints\n-            \n-            Authentication:\n-            - API Key authentication required\n-            - Signature-based request signing\n-            - Rate limiting protection\n-            \n-            Supported Order Types:\n-            - Market orders\n-            - Limit orders\n-            - Stop orders\n-            - Stop-limit orders\n-            \n-            Risk Management:\n-            - Position limits\n-            - Daily loss limits\n-            - Margin requirements\n-            - Liquidation protection\n-            \"\"\"\n-            \n-            doc = ProcessedDocument(\n-                content=extended_api_content,\n-                title=\"Extended Exchange API Documentation\",\n-                category=\"api_documentation\",\n-                subcategory=\"extended_exchange\",\n-                metadata={\n-                    \"platform\": \"extended_exchange\",\n-                    \"doc_type\": \"api_reference\",\n-                    \"importance\": \"critical\",\n-                    \"source\": \"extended_exchange_indexer\"\n-                }\n-            )\n-            \n-            chunks = rag_system._chunk_document(doc)\n-            await rag_system._add_chunks_to_collection(chunks)\n-            docs_count += len(chunks)\n-            \n-        except Exception as e:\n-            logger.error(f\"Failed to index Extended Exchange docs: {str(e)}\")\n-        \n-        return docs_count\n-\n-class X10PythonSDKIndexer(PlatformIndexer):\n-    \"\"\"Indexer for X10 Python SDK documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"x10_python_sdk\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return await self._index_x10_python_docs(rag_system)\n-    \n-    async def _index_x10_python_docs(self, rag_system) -> int:\n-        \"\"\"Index X10 Python SDK documentation\"\"\"\n-        docs_count = 0\n-        try:\n-            x10_sdk_content = \"\"\"\n-            X10 Python SDK Documentation\n-            \n-            The X10 Python SDK provides a comprehensive Python interface for the X10 trading platform.\n-            \n-            Installation:\n-            pip install x10-python-sdk\n-            \n-            Quick Start:\n-            from x10 import Client\n-            \n-            client = Client(api_key=\"your_api_key\", secret=\"your_secret\")\n-            \n-            # Get account balance\n-            balance = client.get_balance()\n-            \n-            # Place an order\n-            order = client.place_order(\n-                symbol=\"BTCUSD\",\n-                side=\"buy\",\n-                quantity=0.1,\n-                price=50000\n-            )\n-            \n-            Key Features:\n-            - Async/await support\n-            - Real-time data streaming\n-            - Order management\n-            - Position tracking\n-            - Risk management tools\n-            - Historical data access\n-            \n-            Error Handling:\n-            - Comprehensive exception handling\n-            - Retry mechanisms\n-            - Rate limit handling\n-            - Connection management\n-            \"\"\"\n-            \n-            doc = ProcessedDocument(\n-                content=x10_sdk_content,\n-                title=\"X10 Python SDK Documentation\",\n-                category=\"sdk_documentation\",\n-                subcategory=\"x10_python\",\n-                metadata={\n-                    \"platform\": \"x10_python_sdk\",\n-                    \"doc_type\": \"sdk_reference\",\n-                    \"importance\": \"high\",\n-                    \"source\": \"x10_python_indexer\"\n-                }\n-            )\n-            \n-            chunks = rag_system._chunk_document(doc)\n-            await rag_system._add_chunks_to_collection(chunks)\n-            docs_count += len(chunks)\n-            \n-        except Exception as e:\n-            logger.error(f\"Failed to index X10 Python SDK docs: {str(e)}\")\n-        \n-        return docs_count\n-\n-class StarknetDartIndexer(PlatformIndexer):\n-    \"\"\"Indexer for Starknet.dart SDK documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"starknet_dart\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return await self._index_starknet_dart_docs(rag_system)\n-    \n-    async def _index_starknet_dart_docs(self, rag_system) -> int:\n-        \"\"\"Index Starknet.dart SDK documentation\"\"\"\n-        docs_count = 0\n-        try:\n-            starknet_dart_content = \"\"\"\n-            Starknet.dart SDK Documentation\n-            \n-            The Starknet.dart SDK enables Flutter/Dart applications to interact with the Starknet blockchain.\n-            \n-            Installation:\n-            dependencies:\n-              starknet: ^latest_version\n-            \n-            Quick Start:\n-            import 'package:starknet/starknet.dart';\n-            \n-            // Initialize provider\n-            final provider = JsonRpcProvider(nodeUrl: 'https://starknet-mainnet.public.blastapi.io');\n-            \n-            // Create account\n-            final account = Account(\n-              provider: provider,\n-              address: 'your_account_address',\n-              keyPair: KeyPair.fromPrivateKey('your_private_key')\n-            );\n-            \n-            Key Features:\n-            - Account management\n-            - Contract interactions\n-            - Transaction signing\n-            - Event filtering\n-            - Type-safe contract calls\n-            - Cairo contract compilation\n-            \n-            Smart Contract Interaction:\n-            - Contract deployment\n-            - Function calls\n-            - Event listening\n-            - State queries\n-            \n-            Security:\n-            - Hardware wallet support\n-            - Secure key management\n-            - Transaction verification\n-            - Network validation\n-            \"\"\"\n-            \n-            doc = ProcessedDocument(\n-                content=starknet_dart_content,\n-                title=\"Starknet.dart SDK Documentation\",\n-                category=\"sdk_documentation\",\n-                subcategory=\"starknet_dart\",\n-                metadata={\n-                    \"platform\": \"starknet_dart\",\n-                    \"doc_type\": \"sdk_reference\",\n-                    \"importance\": \"high\",\n-                    \"source\": \"starknet_dart_indexer\"\n-                }\n-            )\n-            \n-            chunks = rag_system._chunk_document(doc)\n-            await rag_system._add_chunks_to_collection(chunks)\n-            docs_count += len(chunks)\n-            \n-        except Exception as e:\n-            logger.error(f\"Failed to index Starknet.dart SDK docs: {str(e)}\")\n-        \n-        return docs_count\n-\n-class CairoLangIndexer(PlatformIndexer):\n-    \"\"\"Indexer for Cairo language documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"cairo_lang\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return await self._index_cairo_docs(rag_system)\n-    \n-    async def _index_cairo_docs(self, rag_system) -> int:\n-        \"\"\"Index Cairo language documentation\"\"\"\n-        docs_count = 0\n-        try:\n-            cairo_content = \"\"\"\n-            Cairo Language Documentation\n-            \n-            Cairo is a programming language for writing provable programs, where one party can prove to another that \n-            a certain computation was executed correctly.\n-            \n-            Key Concepts:\n-            - Provable computations\n-            - Zero-knowledge proofs\n-            - Starknet smart contracts\n-            - Efficient execution\n-            \n-            Basic Syntax:\n-            #[starknet::contract]\n-            mod HelloStarknet {\n-                #[storage]\n-                struct Storage {\n-                    balance: felt252,\n-                }\n-                \n-                #[external(v0)]\n-                fn increase_balance(ref self: ContractState, amount: felt252) {\n-                    self.balance.write(self.balance.read() + amount);\n-                }\n-                \n-                #[external(v0)]\n-                fn get_balance(self: @ContractState) -> felt252 {\n-                    self.balance.read()\n-                }\n-            }\n-            \n-            Smart Contract Development:\n-            - Contract interfaces\n-            - Storage management\n-            - External functions\n-            - Events and logging\n-            - Access control\n-            \n-            Testing:\n-            - Unit testing framework\n-            - Integration testing\n-            - Deployment testing\n-            - Performance testing\n-            \"\"\"\n-            \n-            doc = ProcessedDocument(\n-                content=cairo_content,\n-                title=\"Cairo Language Documentation\",\n-                category=\"language_documentation\",\n-                subcategory=\"cairo_lang\",\n-                metadata={\n-                    \"platform\": \"cairo_lang\",\n-                    \"doc_type\": \"language_reference\",\n-                    \"importance\": \"high\",\n-                    \"source\": \"cairo_lang_indexer\"\n-                }\n-            )\n-            \n-            chunks = rag_system._chunk_document(doc)\n-            await rag_system._add_chunks_to_collection(chunks)\n-            docs_count += len(chunks)\n-            \n-        except Exception as e:\n-            logger.error(f\"Failed to index Cairo language docs: {str(e)}\")\n-        \n-        return docs_count\n-\n-class AVNUPaymasterIndexer(PlatformIndexer):\n-    \"\"\"Indexer for AVNU Paymaster documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"avnu_paymaster\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return 0  # Placeholder\n-\n-class Web3AuthIndexer(PlatformIndexer):\n-    \"\"\"Indexer for Web3Auth documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"web3auth\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return 0  # Placeholder\n-\n-class ChipiPayIndexer(PlatformIndexer):\n-    \"\"\"Indexer for ChipiPay SDK documentation\"\"\"\n-    \n-    def __init__(self):\n-        super().__init__(\"chipi_pay\")\n-    \n-    async def index_platform_docs(self, rag_system) -> int:\n-        return 0  # Placeholder\n-\n-class DocumentQualityAssessor:\n-    \"\"\"RAGFlow-inspired document quality assessment\"\"\"\n-    \n-    def __init__(self, threshold: float = 0.7, platforms: List[str] = None):\n-        self.threshold = threshold\n-        self.platforms = platforms or []\n-    \n-    def assess_document_quality(self, content: str, metadata: Dict[str, Any]) -> float:\n-        \"\"\"Assess the quality of a document\"\"\"\n-        score = 0.0\n-        \n-        # Content length (25%)\n-        content_length = len(content)\n-        if 100 <= content_length <= 8000:\n-            score += 0.25\n-        elif content_length > 8000:\n-            score += 0.15\n-        elif content_length > 50:\n-            score += 0.1\n-        \n-        # Platform relevance (25%)\n-        platform = metadata.get(\"platform\", \"unknown\")\n-        if platform in self.platforms:\n-            score += 0.25\n-        elif platform != \"unknown\":\n-            score += 0.15\n-        \n-        # Metadata completeness (25%)\n-        required_fields = [\"title\", \"category\", \"doc_type\", \"importance\"]\n-        completeness = sum(1 for field in required_fields if metadata.get(field)) / len(required_fields)\n-        score += completeness * 0.25\n-        \n-        # Content structure (25%)\n-        structure_indicators = [\"#\", \"##\", \"```\", \"*\", \"-\", \"1.\"]\n-        structure_score = min(1.0, sum(1 for indicator in structure_indicators if indicator in content) / len(structure_indicators))\n-        score += structure_score * 0.25\n-        \n-        return min(1.0, score)\n-\n@@ -1207 +97 @@ async def index_documentation(request: IndexRequest, background_tasks: Backgroun\n-    background_tasks.add_task(rag_system.index_sdk_documentation, request.force_reindex)\n+    background_tasks.add_task(rag_system.index_astratrade_documentation, request.force_reindex)\ndiff --git a/knowledge_base/backend/models.py b/knowledge_base/backend/models.py\nnew file mode 100644\nindex 0000000..df4aeb1\n--- /dev/null\n+++ b/knowledge_base/backend/models.py\n@@ -0,0 +1,45 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Pydantic models and data structures for AstraTrade RAG system\n+\"\"\"\n+\n+from typing import List, Optional, Dict, Any\n+from dataclasses import dataclass\n+from pydantic import BaseModel, Field\n+\n+# Request models\n+class QueryRequest(BaseModel):\n+    query: str\n+    max_results: Optional[int] = 5\n+    category: Optional[str] = None\n+    min_similarity: Optional[float] = 0.6\n+\n+class IndexRequest(BaseModel):\n+    force_reindex: bool = False\n+\n+# Response models\n+class QueryResponse(BaseModel):\n+    results: List[Dict[str, Any]]\n+    query_time: float\n+    total_results: int\n+\n+class IndexResponse(BaseModel):\n+    status: str\n+    documents_indexed: int\n+    time_taken: float\n+\n+class StatsResponse(BaseModel):\n+    total_documents: int\n+    categories: Dict[str, int]\n+    last_updated: str\n+    embedding_model: str\n+\n+# Data classes\n+@dataclass\n+class ProcessedDocument:\n+    content: str\n+    title: str\n+    category: str\n+    subcategory: Optional[str]\n+    metadata: Dict[str, Any]\n+    source_url: Optional[str] = None\n\\ No newline at end of file\ndiff --git a/knowledge_base/backend/rag_system.py b/knowledge_base/backend/rag_system.py\nnew file mode 100644\nindex 0000000..440c3a2\n--- /dev/null\n+++ b/knowledge_base/backend/rag_system.py\n@@ -0,0 +1,526 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Core RAG system implementation for AstraTrade\n+\"\"\"\n+\n+import os\n+import asyncio\n+import time\n+import hashlib\n+import logging\n+from typing import List, Optional, Dict, Any\n+from datetime import datetime\n+from pathlib import Path\n+\n+# Vector database and embeddings\n+import chromadb\n+from sentence_transformers import SentenceTransformer\n+from chromadb.utils import embedding_functions\n+\n+# Document processing\n+import requests\n+from langchain.text_splitter import RecursiveCharacterTextSplitter\n+from langchain.schema import Document\n+\n+# Local imports\n+from config import RAG_CONFIG\n+from models import ProcessedDocument\n+\n+# Logging configuration\n+logging.basicConfig(level=logging.INFO)\n+logger = logging.getLogger(__name__)\n+\n+class DocumentQualityAssessor:\n+    \"\"\"RAGFlow-inspired document quality assessment\"\"\"\n+    \n+    def __init__(self, threshold: float = 0.7, platforms: List[str] = None):\n+        self.threshold = threshold\n+        self.platforms = platforms or []\n+    \n+    def assess_document_quality(self, content: str, metadata: Dict[str, Any]) -> float:\n+        \"\"\"Assess the quality of a document\"\"\"\n+        score = 0.0\n+        \n+        # Content length (25%)\n+        content_length = len(content)\n+        if 100 <= content_length <= 8000:\n+            score += 0.25\n+        elif content_length > 8000:\n+            score += 0.15\n+        elif content_length > 50:\n+            score += 0.1\n+        \n+        # Platform relevance (25%)\n+        platform = metadata.get(\"platform\", \"unknown\")\n+        if platform in self.platforms:\n+            score += 0.25\n+        elif platform != \"unknown\":\n+            score += 0.15\n+        \n+        # Metadata completeness (25%)\n+        required_fields = [\"title\", \"category\", \"doc_type\", \"importance\"]\n+        completeness = sum(1 for field in required_fields if metadata.get(field)) / len(required_fields)\n+        score += completeness * 0.25\n+        \n+        # Content structure (25%)\n+        structure_indicators = [\"#\", \"##\", \"```\", \"*\", \"-\", \"1.\"]\n+        structure_score = min(1.0, sum(1 for indicator in structure_indicators if indicator in content) / len(structure_indicators))\n+        score += structure_score * 0.25\n+        \n+        return min(1.0, score)\n+\n+class AstraTradeRAG:\n+    \"\"\"High-performance RAG system for AstraTrade trading platform with RAGFlow features\"\"\"\n+    \n+    def __init__(self):\n+        self.chroma_client = None\n+        self.collection = None\n+        self.embedding_model = None\n+        self.text_splitter = None\n+        self.documents_indexed = 0\n+        self.last_updated = None\n+        self.document_cache = {}          # Cache for processed documents\n+        self.quality_assessor = None      # RAGFlow-inspired quality assessment\n+        self.citation_tracker = {}        # Track citations for grounded answers\n+        self.platform_indexers = {}       # Platform-specific indexers\n+        \n+    async def initialize(self):\n+        \"\"\"Initialize the RAG system with RAGFlow-inspired features\"\"\"\n+        logger.info(\"ðŸš€ Initializing AstraTrade RAG system with advanced features...\")\n+        \n+        # Initialize ChromaDB\n+        self.chroma_client = chromadb.PersistentClient(path=RAG_CONFIG[\"chroma_db_path\"])\n+        \n+        # Initialize embedding model\n+        self.embedding_model = SentenceTransformer(RAG_CONFIG[\"embedding_model\"])\n+        \n+        # Create embedding function for ChromaDB\n+        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n+            model_name=RAG_CONFIG[\"embedding_model\"]\n+        )\n+        \n+        # Get or create collection\n+        try:\n+            self.collection = self.chroma_client.get_collection(\n+                name=RAG_CONFIG[\"collection_name\"],\n+                embedding_function=embedding_function\n+            )\n+            logger.info(f\"âœ… Loaded existing collection: {RAG_CONFIG['collection_name']}\")\n+        except Exception:\n+            self.collection = self.chroma_client.create_collection(\n+                name=RAG_CONFIG[\"collection_name\"],\n+                embedding_function=embedding_function,\n+                metadata={\n+                    \"description\": \"AstraTrade multi-platform trading knowledge base\",\n+                    \"platforms\": RAG_CONFIG[\"platforms\"],\n+                    \"ragflow_features\": {\n+                        \"template_chunking\": RAG_CONFIG[\"template_chunking\"],\n+                        \"grounded_citations\": RAG_CONFIG[\"grounded_citations\"],\n+                        \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n+                    }\n+                }\n+            )\n+            logger.info(f\"âœ… Created new collection: {RAG_CONFIG['collection_name']}\")\n+        \n+        # Initialize text splitter with RAGFlow-inspired improvements\n+        self.text_splitter = RecursiveCharacterTextSplitter(\n+            chunk_size=RAG_CONFIG[\"chunk_size\"],\n+            chunk_overlap=RAG_CONFIG[\"chunk_overlap\"],\n+            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n+            length_function=len,\n+            is_separator_regex=False\n+        )\n+        \n+        # Initialize platform-specific indexers\n+        self._initialize_platform_indexers()\n+        \n+        # Initialize quality assessor\n+        self._initialize_quality_assessor()\n+        \n+        # Get current stats\n+        count = self.collection.count()\n+        self.documents_indexed = count\n+        logger.info(f\"ðŸ“Š Current documents in collection: {count}\")\n+        logger.info(f\"ðŸ”§ RAGFlow features enabled: {list(RAG_CONFIG.keys())}\")\n+        \n+    def _initialize_platform_indexers(self):\n+        \"\"\"Initialize platform-specific indexers for AstraTrade\"\"\"\n+        # Import here to avoid circular imports\n+        from indexers import (\n+            ExtendedExchangeIndexer, X10PythonSDKIndexer, StarknetDartIndexer,\n+            CairoLangIndexer, AVNUPaymasterIndexer, Web3AuthIndexer, ChipiPayIndexer\n+        )\n+        \n+        self.platform_indexers = {\n+            \"extended_exchange\": ExtendedExchangeIndexer(),\n+            \"x10_python_sdk\": X10PythonSDKIndexer(),\n+            \"starknet_dart\": StarknetDartIndexer(),\n+            \"cairo_lang\": CairoLangIndexer(),\n+            \"avnu_paymaster\": AVNUPaymasterIndexer(),\n+            \"web3auth\": Web3AuthIndexer(),\n+            \"chipi_pay\": ChipiPayIndexer()\n+        }\n+        \n+    def _initialize_quality_assessor(self):\n+        \"\"\"Initialize RAGFlow-inspired quality assessment\"\"\"\n+        self.quality_assessor = DocumentQualityAssessor(\n+            threshold=RAG_CONFIG[\"quality_threshold\"],\n+            platforms=RAG_CONFIG[\"platforms\"]\n+        )\n+        \n+    async def index_astratrade_documentation(self, force_reindex: bool = False) -> Dict[str, Any]:\n+        \"\"\"Index all AstraTrade documentation with RAGFlow-inspired features\"\"\"\n+        start_time = datetime.now()\n+        \n+        if force_reindex:\n+            logger.info(\"ðŸ”„ Force reindexing - clearing existing collection...\")\n+            self.collection.delete()\n+            embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n+                model_name=RAG_CONFIG[\"embedding_model\"]\n+            )\n+            self.collection = self.chroma_client.create_collection(\n+                name=RAG_CONFIG[\"collection_name\"],\n+                embedding_function=embedding_function,\n+                metadata={\n+                    \"description\": \"AstraTrade multi-platform trading knowledge base\",\n+                    \"platforms\": RAG_CONFIG[\"platforms\"],\n+                    \"ragflow_features\": {\n+                        \"template_chunking\": RAG_CONFIG[\"template_chunking\"],\n+                        \"grounded_citations\": RAG_CONFIG[\"grounded_citations\"],\n+                        \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n+                    }\n+                }\n+            )\n+        \n+        # Collect documents from all platforms\n+        platform_results = await self._index_all_platforms()\n+        \n+        # Apply RAGFlow-inspired quality assessment\n+        quality_report = await self._assess_collection_quality()\n+        \n+        # Update stats\n+        total_docs = sum(platform_results.values())\n+        self.documents_indexed = total_docs\n+        self.last_updated = datetime.now().isoformat()\n+        \n+        time_taken = (datetime.now() - start_time).total_seconds()\n+        \n+        return {\n+            \"status\": \"completed\",\n+            \"documents_indexed\": total_docs,\n+            \"platform_breakdown\": platform_results,\n+            \"quality_report\": quality_report,\n+            \"ragflow_features\": {\n+                \"template_chunking_applied\": RAG_CONFIG[\"template_chunking\"],\n+                \"grounded_citations_enabled\": RAG_CONFIG[\"grounded_citations\"],\n+                \"deep_doc_understanding\": RAG_CONFIG[\"deep_doc_understanding\"]\n+            },\n+            \"time_taken\": time_taken\n+        }\n+    \n+    async def _index_all_platforms(self) -> Dict[str, int]:\n+        \"\"\"Index documentation from all AstraTrade platforms\"\"\"\n+        platform_results = {}\n+        \n+        for platform_name, indexer in self.platform_indexers.items():\n+            try:\n+                logger.info(f\"ðŸ” Indexing {platform_name} documentation...\")\n+                docs_count = await indexer.index_platform_docs(self)\n+                platform_results[platform_name] = docs_count\n+                logger.info(f\"âœ… Indexed {docs_count} documents from {platform_name}\")\n+            except Exception as e:\n+                logger.error(f\"âŒ Failed to index {platform_name}: {str(e)}\")\n+                platform_results[platform_name] = 0\n+        \n+        return platform_results\n+    \n+    async def _assess_collection_quality(self) -> Dict[str, Any]:\n+        \"\"\"Assess the quality of the indexed collection - RAGFlow inspired\"\"\"\n+        if not self.quality_assessor:\n+            return {\"status\": \"quality_assessor_not_initialized\"}\n+        \n+        try:\n+            # Get sample of documents for quality assessment\n+            sample_results = self.collection.get(limit=100, include=[\"documents\", \"metadatas\"])\n+            \n+            quality_metrics = {\n+                \"total_documents\": len(sample_results[\"documents\"]),\n+                \"platform_coverage\": {},\n+                \"chunk_quality_distribution\": {},\n+                \"citation_completeness\": 0.0,\n+                \"template_coverage\": 0.0\n+            }\n+            \n+            # Analyze platform coverage\n+            for metadata in sample_results[\"metadatas\"]:\n+                platform = metadata.get(\"platform\", \"unknown\")\n+                quality_metrics[\"platform_coverage\"][platform] = quality_metrics[\"platform_coverage\"].get(platform, 0) + 1\n+            \n+            # Analyze chunk quality\n+            quality_scores = []\n+            for doc, metadata in zip(sample_results[\"documents\"], sample_results[\"metadatas\"]):\n+                quality_score = self.quality_assessor.assess_document_quality(doc, metadata)\n+                quality_scores.append(quality_score)\n+            \n+            if quality_scores:\n+                quality_metrics[\"average_quality_score\"] = sum(quality_scores) / len(quality_scores)\n+                quality_metrics[\"high_quality_percentage\"] = len([s for s in quality_scores if s > 0.8]) / len(quality_scores) * 100\n+            \n+            return quality_metrics\n+            \n+        except Exception as e:\n+            logger.error(f\"Quality assessment failed: {str(e)}\")\n+            return {\"status\": \"assessment_failed\", \"error\": str(e)}\n+    \n+    async def _collect_astratrade_documentation(self) -> List[ProcessedDocument]:\n+        \"\"\"Collect all AstraTrade platform documentation with RAGFlow-inspired deep understanding\"\"\"\n+        documents = []\n+        \n+        # Manual documentation files\n+        documents.extend(await self._fetch_manual_docs())\n+        \n+        # Extended Exchange API documentation\n+        documents.extend(await self._fetch_extended_exchange_docs())\n+        \n+        # X10 Python SDK documentation\n+        documents.extend(await self._fetch_x10_python_docs())\n+        \n+        # Starknet.dart SDK documentation\n+        documents.extend(await self._fetch_starknet_dart_docs())\n+        \n+        # Cairo language documentation\n+        documents.extend(await self._fetch_cairo_docs())\n+        \n+        # AVNU Paymaster documentation\n+        documents.extend(await self._fetch_avnu_paymaster_docs())\n+        \n+        # Web3Auth documentation\n+        documents.extend(await self._fetch_web3auth_docs())\n+        \n+        # ChipiPay SDK documentation\n+        documents.extend(await self._fetch_chipi_pay_docs())\n+        \n+        return documents\n+    \n+    async def _fetch_manual_docs(self) -> List[ProcessedDocument]:\n+        \"\"\"Fetch manual documentation files from the docs folder\"\"\"\n+        documents = []\n+        docs_path = Path(\"../docs/manual_docs\")\n+        \n+        if docs_path.exists():\n+            for doc_file in docs_path.glob(\"*.md\"):\n+                try:\n+                    content = doc_file.read_text(encoding='utf-8')\n+                    \n+                    # Apply deep document understanding\n+                    doc_type = self._detect_document_type(content, doc_file.name)\n+                    importance = self._assess_document_importance(content, doc_file.name)\n+                    \n+                    documents.append(ProcessedDocument(\n+                        content=content,\n+                        title=doc_file.stem.replace('_', ' ').title(),\n+                        category=doc_type[\"category\"],\n+                        subcategory=doc_type[\"subcategory\"],\n+                        metadata={\n+                            \"source\": \"manual_docs\",\n+                            \"file_path\": str(doc_file),\n+                            \"importance\": importance,\n+                            \"doc_type\": doc_type[\"type\"],\n+                            \"platform\": doc_type[\"platform\"],\n+                            \"last_modified\": doc_file.stat().st_mtime\n+                        }\n+                    ))\n+                    \n+                except Exception as e:\n+                    logger.error(f\"Failed to process {doc_file}: {str(e)}\")\n+        \n+        return documents\n+    \n+    def _detect_document_type(self, content: str, filename: str) -> Dict[str, str]:\n+        \"\"\"Detect document type using RAGFlow-inspired analysis\"\"\"\n+        content_lower = content.lower()\n+        filename_lower = filename.lower()\n+        \n+        # Platform detection\n+        if \"extended\" in filename_lower and \"api\" in filename_lower:\n+            return {\n+                \"category\": \"api_documentation\",\n+                \"subcategory\": \"extended_exchange\",\n+                \"type\": \"api_reference\",\n+                \"platform\": \"extended_exchange\"\n+            }\n+        elif \"x10\" in filename_lower or \"python\" in filename_lower:\n+            return {\n+                \"category\": \"sdk_documentation\",\n+                \"subcategory\": \"x10_python\",\n+                \"type\": \"sdk_reference\",\n+                \"platform\": \"x10_python_sdk\"\n+            }\n+        elif \"starknet\" in filename_lower or \"dart\" in filename_lower:\n+            return {\n+                \"category\": \"sdk_documentation\",\n+                \"subcategory\": \"starknet_dart\",\n+                \"type\": \"sdk_reference\",\n+                \"platform\": \"starknet_dart\"\n+            }\n+        elif \"cairo\" in filename_lower:\n+            return {\n+                \"category\": \"language_documentation\",\n+                \"subcategory\": \"cairo_lang\",\n+                \"type\": \"language_reference\",\n+                \"platform\": \"cairo_lang\"\n+            }\n+        elif \"avnu\" in filename_lower or \"paymaster\" in filename_lower:\n+            return {\n+                \"category\": \"integration_documentation\",\n+                \"subcategory\": \"avnu_paymaster\",\n+                \"type\": \"integration_guide\",\n+                \"platform\": \"avnu_paymaster\"\n+            }\n+        elif \"web3auth\" in filename_lower:\n+            return {\n+                \"category\": \"authentication_documentation\",\n+                \"subcategory\": \"web3auth\",\n+                \"type\": \"auth_reference\",\n+                \"platform\": \"web3auth\"\n+            }\n+        elif \"chipi\" in filename_lower:\n+            return {\n+                \"category\": \"payment_documentation\",\n+                \"subcategory\": \"chipi_pay\",\n+                \"type\": \"payment_reference\",\n+                \"platform\": \"chipi_pay\"\n+            }\n+        else:\n+            return {\n+                \"category\": \"general_documentation\",\n+                \"subcategory\": \"general\",\n+                \"type\": \"general_reference\",\n+                \"platform\": \"general\"\n+            }\n+    \n+    def _assess_document_importance(self, content: str, filename: str) -> str:\n+        \"\"\"Assess document importance using RAGFlow-inspired analysis\"\"\"\n+        content_lower = content.lower()\n+        \n+        # Critical indicators\n+        critical_indicators = [\"api reference\", \"getting started\", \"authentication\", \"security\", \"deployment\"]\n+        if any(indicator in content_lower for indicator in critical_indicators):\n+            return \"critical\"\n+        \n+        # High importance indicators\n+        high_indicators = [\"integration\", \"configuration\", \"examples\", \"tutorial\", \"guide\"]\n+        if any(indicator in content_lower for indicator in high_indicators):\n+            return \"high\"\n+        \n+        # Medium importance indicators\n+        medium_indicators = [\"reference\", \"documentation\", \"sdk\", \"api\"]\n+        if any(indicator in content_lower for indicator in medium_indicators):\n+            return \"medium\"\n+        \n+        return \"low\"\n+    \n+    def _chunk_document(self, doc: ProcessedDocument) -> List[Dict[str, Any]]:\n+        \"\"\"Chunk a document into smaller pieces\"\"\"\n+        chunks = self.text_splitter.split_text(doc.content)\n+        \n+        chunked_docs = []\n+        for i, chunk in enumerate(chunks):\n+            chunk_id = f\"{doc.category}_{doc.subcategory}_{i}\" if doc.subcategory else f\"{doc.category}_{i}\"\n+            \n+            chunked_docs.append({\n+                \"id\": chunk_id,\n+                \"content\": chunk,\n+                \"metadata\": {\n+                    \"title\": doc.title,\n+                    \"category\": doc.category,\n+                    \"subcategory\": doc.subcategory,\n+                    \"chunk_index\": i,\n+                    \"total_chunks\": len(chunks),\n+                    \"source_url\": doc.source_url,\n+                    **doc.metadata\n+                }\n+            })\n+        \n+        return chunked_docs\n+    \n+    async def _add_chunks_to_collection(self, chunks: List[Dict[str, Any]]):\n+        \"\"\"Add document chunks to ChromaDB collection\"\"\"\n+        batch_size = 100\n+        \n+        for i in range(0, len(chunks), batch_size):\n+            batch = chunks[i:i + batch_size]\n+            \n+            ids = [chunk[\"id\"] for chunk in batch]\n+            documents = [chunk[\"content\"] for chunk in batch]\n+            metadatas = [chunk[\"metadata\"] for chunk in batch]\n+            \n+            self.collection.add(\n+                ids=ids,\n+                documents=documents,\n+                metadatas=metadatas\n+            )\n+            \n+            print(f\"âœ… Added batch {i//batch_size + 1}/{(len(chunks) + batch_size - 1)//batch_size}\")\n+    \n+    async def search(self, query: str, max_results: int = 5, category: str = None, \n+                    min_similarity: float = 0.6) -> Dict[str, Any]:\n+        \"\"\"Search the knowledge base\"\"\"\n+        start_time = datetime.now()\n+        \n+        # Prepare query filters\n+        where_filter = {}\n+        if category:\n+            where_filter[\"category\"] = category\n+        \n+        # Perform vector search\n+        results = self.collection.query(\n+            query_texts=[query],\n+            n_results=max_results,\n+            where=where_filter if where_filter else None\n+        )\n+        \n+        # Process results\n+        processed_results = []\n+        if results[\"documents\"] and results[\"documents\"][0]:\n+            for i, doc in enumerate(results[\"documents\"][0]):\n+                metadata = results[\"metadatas\"][0][i] if results[\"metadatas\"] else {}\n+                distance = results[\"distances\"][0][i] if results[\"distances\"] else 0\n+                similarity = 1 - distance  # Convert distance to similarity\n+                \n+                if similarity >= min_similarity:\n+                    processed_results.append({\n+                        \"content\": doc,\n+                        \"title\": metadata.get(\"title\", \"Unknown\"),\n+                        \"category\": metadata.get(\"category\", \"Unknown\"),\n+                        \"subcategory\": metadata.get(\"subcategory\"),\n+                        \"similarity\": similarity,\n+                        \"source_url\": metadata.get(\"source_url\"),\n+                        \"metadata\": metadata\n+                    })\n+        \n+        query_time = (datetime.now() - start_time).total_seconds()\n+        \n+        return {\n+            \"results\": processed_results,\n+            \"query_time\": query_time,\n+            \"total_results\": len(processed_results)\n+        }\n+    \n+    def get_stats(self) -> Dict[str, Any]:\n+        \"\"\"Get knowledge base statistics\"\"\"\n+        total_docs = self.collection.count()\n+        \n+        # Get categories\n+        all_metadata = self.collection.get()[\"metadatas\"]\n+        categories = {}\n+        for metadata in all_metadata:\n+            category = metadata.get(\"category\", \"unknown\")\n+            categories[category] = categories.get(category, 0) + 1\n+        \n+        return {\n+            \"total_documents\": total_docs,\n+            \"categories\": categories,\n+            \"last_updated\": self.last_updated or \"Never\",\n+            \"embedding_model\": RAG_CONFIG[\"embedding_model\"]\n+        }\n\\ No newline at end of file",
  "metadata": {
    "type": "commit",
    "special_code": "209e0ff2bcd94a7272192501cc2518039ad88bfb",
    "author": "Peter",
    "date": "2025-07-11 18:46:27 +0700"
  }
}